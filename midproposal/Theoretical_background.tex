\chapter{THEORETICAL BACKGROUND}

\section{What is LLaMA-2?}
LLaMA-2 (Large Language Model Meta AI) is a family of large-scale transformer-based
language models developed by Meta. It is designed to generate human-like text by
learning statistical patterns from large corpora of textual data. LLaMA-2 models are
available in different parameter sizes, ranging from 7 billion to 70 billion parameters,
and are optimized for both research and practical applications. Due to their open
availability and strong performance, LLaMA-2 models are widely used for tasks such as
text generation, question answering, summarization, and code generation.

\subsection{LLaMA Architecture Variation}
LLaMA-2 is built upon the transformer-based architecture, which utilizes
self-attention mechanisms to effectively capture long-range dependencies
within sequential data. The model is composed of multiple stacked decoder
layers, where each layer integrates a self-attention module followed by a
position-wise feed-forward neural network. To enhance training stability and
ensure efficient gradient flow, residual connections and layer normalization
are employed throughout the network. This architectural design allows for
highly parallel computation and scalability, making LLaMA-2 suitable for
training large-scale language models with billions of parameters.

\begin{table}[h]
\centering
\caption{Architectural Variants of LLaMA-2 Models}
\label{tab:llama2_variants}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Model Size} & \textbf{Parameter Size} & \textbf{Training Tokens} & \textbf{Attention Heads} & \textbf{Layers} \\
\hline
LLaMA-2 7B  & 7 Billion  & $\sim$2 Trillion & 32 & 32 \\
LLaMA-2 13B & 13 Billion & $\sim$2 Trillion & 40 & 40 \\
LLaMA-2 70B & 70 Billion & $\sim$2 Trillion & 64 & 80 \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Context Length of LLaMA-2 Models}
\label{tab:llama2_context}
\begin{tabular}{|c|c|}
\hline
\textbf{Model Size} & \textbf{Context Length (Tokens)} \\
\hline
LLaMA-2 7B  & 4096 \\
LLaMA-2 13B & 4096 \\
LLaMA-2 70B & 4096 \\
\hline
\end{tabular}
\end{table}

\subsection{Transformer Architecture Analysis}
The LLaMA-2 model adopts a decoder-only transformer architecture, which is
specifically designed for autoregressive language modeling. In this architecture,
each decoder layer processes input tokens sequentially while attending to all
previous tokens through a masked self-attention mechanism. This causal masking
ensures that the prediction of a token depends only on preceding context, making
the model suitable for text and code generation tasks.

Each decoder layer consists of a multi-head self-attention module followed by a
position-wise feed-forward network. The multi-head attention mechanism enables
the model to capture diverse linguistic and semantic relationships by attending
to information from multiple representation subspaces. Residual connections and
layer normalization are incorporated to stabilize training, improve convergence,
and facilitate efficient gradient propagation across deep network layers.

In summary, LLaMA-2 employs a \textit{decoder-only transformer architecture with
masked multi-head self-attention, residual connections, and layer normalization},
making it well-suited for large-scale language modeling and generative tasks.

\subsection{Pre-training of LLaMA-2}
The pre-training of LLaMA-2 is conducted on a large-scale corpus composed of a
new mixture of data collected exclusively from publicly available sources.
The training corpus consists of approximately two trillion tokens, representing a
carefully selected performanceâ€“cost trade-off. To improve factual accuracy and
reduce hallucinations, more reliable and knowledge-dense sources were
up-sampled during training.

Pre-training follows an autoregressive language modeling objective, where the
model learns to predict the next token given all preceding tokens in a sequence.
This objective aligns with the decoder-only transformer architecture and enables
the model to acquire strong generative capabilities. Extensive data analysis was
performed during pre-training to better understand the strengths and
limitations of the resulting models, as reported in the LLaMA-2 evaluation
studies.

\subsubsection{Training Configuration}
LLaMA-2 largely adopts the pre-training setup and architectural design of LLaMA-1,
while introducing several key improvements. The model
uses a standard transformer architecture with
pre-normalization implemented via RMSNorm. The SwiGLU
activation function is employed within feed-forward layers,
and rotary positional embeddings (RoPE) are used to encode
positional information.The primary architectural enhancements over LLaMA-1 include an increased context
length of 4096 tokens.

\subsection{Input Sequence Representation for LLaMA-2 7B Chat Model}

The LLaMA-2 7B Chat model uses a structured input format that distinguishes system prompts, user instructions, and responses using special tokens. The general input format is:

\[
\mathbf{X} = \langle s \rangle \; [\text{INST}] \; \ll \text{SYS} \gg \, S \, \ll / \text{SYS} \gg \, I \; [/\text{INST}] \; R \; \langle /s \rangle
\]

where:

\begin{itemize}
    \item $\langle s \rangle$ : start-of-sequence token  
    \item $[\text{INST}]$ : start of instruction  
    \item $\ll \text{SYS} \gg S \ll / \text{SYS} \gg$ : system prompt $S$ enclosed in system tags  
    \item $I$ : user instruction  
    \item $[/\text{INST}]$ : end of instruction  
    \item $R$ : model response or target output (e.g., code block)  
    \item $\langle /s \rangle$ : end-of-sequence token
\end{itemize}

\subsubsection*{Example}

Consider the following system prompt $S$, user instruction $I$, and expected code $R$:

\[
\begin{aligned}
S &= \text{"You are a Python code assistant."} \\
I &= \text{"Write a function to compute factorial."} \\
R &= \text{"def factorial(n): \ldots"}
\end{aligned}
\]

The input sequence is constructed as:

\[
\mathbf{X} =
\langle s \rangle 
\; [\text{INST}] 
\; \ll \text{SYS} \gg \text{You are a Python code assistant.} \ll / \text{SYS} \gg 
\; \text{Write a function to compute factorial.} 
\; [/ \text{INST}] 
\; \text{```def factorial(n): ...```} 
\; \langle /s \rangle
\]



\subsubsection{Tokenizer and Vocabulary}
LLaMA-2 utilizes the same tokenizer as LLaMA-1, based on Byte Pair Encoding (BPE)
implemented using SentencePiece. The tokenizer
splits numerical values into individual digits and decomposes unknown UTF-8
characters into byte-level representations. The resulting vocabulary consists
of 32,000 tokens.
\subsubsection{Example:}
To illustrate the BPE algorithm, consider a simplified example with a tiny
corpus containing the words: \texttt{low}, \texttt{lower}, \texttt{newest}, and \texttt{widest}.

\begin{table}[h]
\centering
\caption{Step-by-step BPE token merges}
\label{tab:bpe_example}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Step} & \textbf{Symbol Pairs Merged} & \textbf{Resulting Tokens} \\
\hline
0 & - & l o w, l o w e r, n e w e s t, w i d e s t \\
1 & e s & l o w, l o w e r, n e \textbf{es} t, w i d e \textbf{es} t \\
2 & es t & l o w, l o w e r, n e \textbf{es t}, w i d e \textbf{es t} \\
3 & l o & \textbf{lo} w, \textbf{lo} w e r, n e es t, w i d e es t \\
4 & w e & lo \textbf{w e} r, n e es t, w i d e es t \\
\hline
\end{tabular}
\end{table}

\noindent
After several iterations, frequently occurring subword units such as \texttt{lo}, 
\texttt{w e}, and \texttt{es t} are merged, resulting in a subword vocabulary that
can efficiently represent all words in the corpus, including rare or unseen words.

\subsection{Rotary Positional Embeddings (RoPE)}
Rotary Positional Embeddings (RoPE) is a technique used to encode positional
information in transformer models. Unlike traditional positional encodings that
are added to token embeddings, RoPE applies a rotation to the query and key vectors
based on their positions. This rotation is achieved using complex numbers, where
the real and imaginary parts correspond to cosine and sine functions of the position.
RoPE allows the model to capture relative positional relationships more effectively,
enabling it to generalize better to longer sequences beyond the training context length.

\subsubsection*{Mathematical Formulation}
Let $\mathbf{q}, \mathbf{k} \in \mathbb{R}^d$ denote the query and key vectors of
a transformer attention layer, and let $p$ be the token position. RoPE applies a
rotation to each pair of dimensions $(q_{2i-1}, q_{2i})$ and $(k_{2i-1}, k_{2i})$ as follows:

\begin{equation}
\begin{bmatrix}
q'_{2i-1} \\
q'_{2i}
\end{bmatrix}
=
\begin{bmatrix}
\cos(\theta_p) & -\sin(\theta_p) \\
\sin(\theta_p) & \cos(\theta_p)
\end{bmatrix}
\begin{bmatrix}
q_{2i-1} \\
q_{2i}
\end{bmatrix}, 
\quad
\begin{bmatrix}
k'_{2i-1} \\
k'_{2i}
\end{bmatrix}
=
\begin{bmatrix}
\cos(\theta_p) & -\sin(\theta_p) \\
\sin(\theta_p) & \cos(\theta_p)
\end{bmatrix}
\begin{bmatrix}
k_{2i-1} \\
k_{2i}
\end{bmatrix}
\end{equation}

Here, $\theta_p$ is the rotation angle associated with position $p$:

\begin{equation}
\theta_p = \frac{10000^{-2(i-1)/d}}{p}
\end{equation}

After rotation, the transformed queries $\mathbf{q}'$ and keys $\mathbf{k}'$ are
used in the standard attention computation:

\begin{equation}
\text{Attention}(\mathbf{Q}', \mathbf{K}', \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}' (\mathbf{K}')^\top}{\sqrt{d_k}}\right) \mathbf{V}
\end{equation}

This rotation allows the attention mechanism to **implicitly encode relative positions**
between tokens, improving the model's ability to generalize to sequences longer
than those seen during training.

\subsection{Multi-Head Attention}
Multi-Head Attention (MHA) is a key mechanism in transformer architectures  
and is used in LLaMA-2 7B and 13B models. MHA allows the model to attend to different positions 
and representation subspaces simultaneously, capturing complex relationships between tokens.

Given input embeddings $\mathbf{X} \in \mathbb{R}^{n \times d}$, where $n$ is the number of tokens 
and $d$ is the hidden dimension, MHA first computes the query, key, and value matrices:

\[
\mathbf{Q} = \mathbf{X} W^Q, \quad
\mathbf{K} = \mathbf{X} W^K, \quad
\mathbf{V} = \mathbf{X} W^V
\]

where $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ are learned projection matrices, and $d_k = d/h$ 
for $h$ attention heads.

For each head $i = 1, \dots, h$, scaled dot-product attention is:

\[
\text{head}_i = \text{Attention}(\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i) =
\text{softmax}\left(\frac{\mathbf{Q}_i \mathbf{K}_i^\top}{\sqrt{d_k}}\right) \mathbf{V}_i
\]

The outputs of all heads are concatenated and projected:

\[
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
\text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
\]

where $W^O \in \mathbb{R}^{d \times d}$ is a learned output matrix.

---

\subsection{Activation Function (SwiGLU)}
LLaMA-2 uses the SwiGLU activation function \cite{shazeer2020glu}, a variant of the Gated Linear Unit (GLU) 
that improves model expressivity and training stability. The SwiGLU activation is applied in the feed-forward 
network of each transformer layer.

Given an input vector $\mathbf{x} \in \mathbb{R}^d$, SwiGLU splits it into two linear projections:

\[
\mathbf{a} = \mathbf{x} W_1, \quad \mathbf{b} = \mathbf{x} W_2
\]

where $W_1, W_2 \in \mathbb{R}^{d \times d_{ff}}$, with $d_{ff}$ being the hidden dimension of the feed-forward layer.

The SwiGLU output is computed as:

\[
\text{SwiGLU}(\mathbf{x}) = \mathbf{a} \odot \text{SiLU}(\mathbf{b})
\]

where $\odot$ denotes element-wise multiplication, and SiLU (Sigmoid Linear Unit) is:

\[
\text{SiLU}(z) = z \cdot \sigma(z) = \frac{z}{1 + e^{-z}}
\]

SwiGLU introduces non-linearity while gating part of the input, allowing the network to better model complex patterns.

---

\subsection{RMSNorm}
RMSNorm (Root Mean Square Layer Normalization) is a normalization technique \cite{zhang2019rmsnorm} 
used in LLaMA-2 to stabilize training and improve convergence. Unlike LayerNorm, RMSNorm does not subtract 
the mean, reducing computational overhead.

For an input vector $\mathbf{x} \in \mathbb{R}^d$, RMSNorm computes the root-mean-square of the elements:

\[
\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2}
\]

The normalized output is then:

\[
\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\text{RMS}(\mathbf{x})} \cdot g
\]

where $g \in \mathbb{R}^d$ is a learned gain parameter. RMSNorm improves gradient flow and training stability 
while being computationally cheaper than standard LayerNorm.


---

\section{Fine-Tuning}

Fine-tuning is the process of adapting a pre-trained language model to a specific task or domain by continuing training on a smaller, task-specific dataset. Instead of training a model from scratch, fine-tuning leverages the general knowledge learned during pre-training, reducing computational cost and data requirements.


Let a pre-trained model have parameters $\theta \in \mathbb{R}^d$, trained on a large general corpus $\mathcal{D}_{\text{pretrain}}$. The model defines a probability distribution over token sequences:

\[
P_\theta(y \mid x) \quad \text{for input sequence } x \text{ and target sequence } y
\]

For a downstream task with dataset $\mathcal{D}_{\text{task}} = \{(x_i, y_i)\}_{i=1}^N$, fine-tuning adjusts the parameters $\theta$ to minimize a task-specific loss function $\mathcal{L}$, typically the cross-entropy loss:

\[
\mathcal{L}(\theta) = - \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T_i} \log P_\theta(y_{i,t} \mid y_{i,<t}, x_i)
\]

where:  
- $y_{i,t}$ is the $t$-th token of the target sequence $y_i$,  
- $y_{i,<t}$ denotes all previous tokens in the sequence,  
- $T_i$ is the length of sequence $y_i$.  

The parameters are updated using gradient descent:

\[
\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta)
\]

where $\eta$ is the learning rate.

---

\section{Quantized Low-Rank Adaptation (QLoRA)}

Quantized Low-Rank Adaptation (QLoRA) is a parameter-efficient fine-tuning technique that combines
weight quantization with low-rank adaptation (LoRA). This allows large language models like LLaMA-2
to be fine-tuned on limited hardware while keeping memory usage low.

\subsection*{Mathematical Formulation}

\textbf{1. Weight Quantization:}  
Let $\mathbf{W} \in \mathbb{R}^{d \times k}$ be the weight matrix of a transformer layer.  
In QLoRA, $\mathbf{W}$ is quantized to a low-bit representation (e.g., 4-bit) to reduce memory:

\[
\mathbf{W}_q = Q(\mathbf{W}) \approx \mathbf{W}
\]

where $Q(\cdot)$ denotes a quantization function that maps full-precision weights to low-bit integers while
preserving approximate values. This reduces the storage requirement from $32$-bit per weight to $4$-bit.

\bigskip

\textbf{2. Low-Rank Adaptation (LoRA):}  
Instead of updating the full weight matrix $\mathbf{W}_q$ during fine-tuning, LoRA adds trainable low-rank matrices
$\mathbf{A} \in \mathbb{R}^{d \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$:

\[
\mathbf{W}_{\text{adapted}} = \mathbf{W}_q + \Delta \mathbf{W}, \quad 
\Delta \mathbf{W} = \mathbf{A} \mathbf{B}
\]

Here, $\Delta \mathbf{W}$ captures task-specific updates in a low-dimensional subspace. Only $\mathbf{A}$ and $\mathbf{B}$ are trained, while $\mathbf{W}_q$ remains frozen.

\bigskip

\textbf{3. Forward Pass with QLoRA:}  
During the forward pass, the adapted weight is used in the standard linear layer:

\[
\mathbf{y} = (\mathbf{W}_q + \mathbf{A}\mathbf{B}) \mathbf{x} + \mathbf{b}
\]

where $\mathbf{x} \in \mathbb{R}^k$ is the input, $\mathbf{y} \in \mathbb{R}^d$ is the output, and
$\mathbf{b} \in \mathbb{R}^d$ is the bias.  

\bigskip

\textbf{4. Training Efficiency:}  
- Memory usage is reduced because $\mathbf{W}_q$ is quantized and only the small matrices $\mathbf{A}$ and $\mathbf{B}$ are stored in full precision.  
- Computational cost is lowered since rank $r \ll d,k$.  
- Fine-tuning becomes feasible even on GPUs with limited VRAM.

\bigskip

In summary, QLoRA combines low-bit quantization of the original weights with trainable low-rank updates, enabling large models to be fine-tuned efficiently:

\[
\boxed{\mathbf{W}_{\text{QLoRA}} = Q(\mathbf{W}) + \mathbf{A}\mathbf{B}}
\]

This is the key formulation used in LLaMA-2 fine-tuning for resource-constrained hardware.


---

\section{Code Generation with LLMs}

Large Language Models (LLMs) such as LLaMA-2 have demonstrated strong capabilities in automatic code generation. 
They learn programming syntax, semantics, and patterns from large-scale code datasets and can generate source code 
from natural language instructions, complete partial code snippets, and assist in debugging.

\subsection*{Mathematical Perspective}

Let an input instruction in natural language be denoted as:

\[
\text{Instruction} = \text{"Write a Python function to compute factorial"}
\]

1. \textbf{Tokenization:}  
The instruction is first tokenized using a tokenizer (e.g., Byte-Pair Encoding), mapping the input string to a sequence of tokens:

\[
\mathbf{x} = [x_1, x_2, \dots, x_n], \quad x_i \in V
\]

where $V$ is the vocabulary of the model. For example:

\[
\text{"Write a Python function"} \longrightarrow [101, 523, 324, 76, 981]
\]

2. \textbf{Autoregressive Generation:}  
The model generates code tokens one by one using an autoregressive approach:

\[
P(x_{t+1} \mid x_1, x_2, \dots, x_t) = \text{softmax}\left(\frac{\mathbf{Q}_t \mathbf{K}_t^\top}{\sqrt{d_k}} \right) \mathbf{V}_t
\]

Here:  
- $\mathbf{Q}_t, \mathbf{K}_t, \mathbf{V}_t$ are the query, key, and value matrices computed from the tokens in the transformer layer.  
- $x_{t+1}$ is the next token predicted based on all previous tokens $x_1, \dots, x_t$.  

3. \textbf{Sequence Formation:}  
The predicted token is appended to the input sequence:

\[
\mathbf{x} \leftarrow [x_1, x_2, \dots, x_t, x_{t+1}]
\]

This process is repeated until a special end-of-sequence token $<\text{EOS}>$ is generated.  

\subsection*{Example}

\textbf{Input Instruction:}  
\texttt{"Write a Python function to compute factorial"}

\textbf{Tokenized Input:}  
\[
[x_1, x_2, x_3, x_4, x_5] = [101, 523, 324, 76, 981]
\]

\textbf{Step-by-step Token Generation:}  

\[
\begin{aligned}
x_6 &\sim P(x_6 \mid x_1, \dots, x_5) \longrightarrow \texttt{def} \\
x_7 &\sim P(x_7 \mid x_1, \dots, x_6) \longrightarrow \texttt{factorial} \\
x_8 &\sim P(x_8 \mid x_1, \dots, x_7) \longrightarrow \texttt{(n):} \\
x_9 &\sim P(x_9 \mid x_1, \dots, x_8) \longrightarrow \texttt{if} \\
& \vdots \\
x_{15} &\sim P(x_{15} \mid x_1, \dots, x_{14}) \longrightarrow \texttt{return n*factorial(n-1)}
\end{aligned}
\]

\textbf{Generated Code:}

\begin{verbatim}
def factorial(n):
    if n == 0 or n == 1:
        return 1
    else:
        return n * factorial(n-1)
\end{verbatim}


From a mathematical perspective, code generation is an autoregressive token prediction task, where the model estimates the probability distribution of the next token conditioned on all previous tokens. Fine-tuning LLMs on code datasets allows them to better capture programming patterns, syntax, and semantics, enabling practical applications in software development, automated code completion, and educational tools.
