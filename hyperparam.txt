 Trial 0 finished with value: 1.4871132373809814 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 3, 'learning_rate': 3.986811184660196e-05, 'r': 64, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 1 finished with value: 0.7335254549980164 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 0, 'learning_rate': 0.00037003809984736415, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
 Trial 2 finished with value: 0.7669356465339661 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 3, 'learning_rate': 9.938397041096432e-05, 'r': 64, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 3 finished with value: 1.8330204486846924 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 4, 'learning_rate': 1.141819103975943e-05, 'r': 64, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 4 finished with value: 1.5336822271347046 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 2, 'learning_rate': 6.306286946788976e-05, 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 5 finished with value: 0.7575558423995972 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 2, 'learning_rate': 0.0001729630881709539, 'r': 64, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
 Trial 6 finished with value: 1.4349465370178223 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 3, 'learning_rate': 5.589217379428488e-05, 'r': 32, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 7 finished with value: 1.5084116458892822 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 4, 'learning_rate': 0.00014796060354946012, 'r': 8, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 8 finished with value: 1.761817216873169 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 0, 'learning_rate': 1.4948347198607013e-05, 'r': 32, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 9 finished with value: 1.321707010269165 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 1, 'learning_rate': 5.047576253783951e-05, 'r': 64, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 10 finished with value: 0.7399393916130066 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 0, 'learning_rate': 0.0007841189036350719, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 11 finished with value: 0.7607496380805969 and parameters: {'per_device_train_batch_size': 8, 'warmup_steps': 0, 'learning_rate': 0.0008366260949083839, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 12 finished with value: 0.7396601438522339 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 1, 'learning_rate': 0.0007890996448116656, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 13 finished with value: 0.8433547019958496 and parameters: {'per_device_train_batch_size': 8, 'warmup_steps': 1, 'learning_rate': 0.0003377745931750341, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 14 finished with value: 0.7628464698791504 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 1, 'learning_rate': 0.0004321841805049895, 'r': 8, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}
  Trial 15 finished with value: 0.7330776453018188 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 5, 'learning_rate': 0.0003443636570165762, 'r': 16, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 16 finished with value: 0.7166050672531128 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 5, 'learning_rate': 0.00031287503882560544, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}
   Trial 17 finished with value: 0.7194110751152039 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 5, 'learning_rate': 0.00021151571890294502, 'r': 16, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
   Trial 18 finished with value: 0.7313404083251953 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 5, 'learning_rate': 0.00014198820139959268, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
    Trial 19 finished with value: 0.7131057977676392 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 4, 'learning_rate': 0.00026491674059880415, 'r': 32, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 19 with value: 0.7131057977676392.


    Best hyperparameters: {'per_device_train_batch_size': 3, 'warmup_steps': 4, 'learning_rate': 0.00026491674059880415, 'r': 32, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}



Trial 0 finished with value: 0.7511105537414551 and parameters: {'per_device_train_batch_size': 15, 'warmup_steps': 1, 'learning_rate': 0.00038916315600967283, 'r': 64, 'lora_alpha': 16, 'target_modules': ['v_proj', 'k_proj']}. Best is trial 0 with value: 0.7511105537414551.
Trial 1 finished with value: 0.7325910925865173 and parameters: {'per_device_train_batch_size': 13, 'warmup_steps': 9, 'learning_rate': 0.00043482996384774496, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 1 with value: 0.7325910925865173.
Trial 2 finished with value: 0.7376483082771301 and parameters: {'per_device_train_batch_size': 11, 'warmup_steps': 2, 'learning_rate': 0.00035324135170089406, 'r': 32, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 1 with value: 0.7325910925865173.
Trial 3 finished with value: 0.8122608661651611 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 3, 'learning_rate': 2.609609725569948e-05, 'r': 32, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 1 with value: 0.7325910925865173.
Trial 4 finished with value: 0.7810811400413513 and parameters: {'per_device_train_batch_size': 8, 'warmup_steps': 3, 'learning_rate': 8.487335505566941e-05, 'r': 16, 'lora_alpha': 16, 'target_modules': ['v_proj', 'k_proj']}. Best is trial 1 with value: 0.7325910925865173.
Trial 5 finished with value: 0.7260868549346924 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 7, 'learning_rate': 0.0003737918942333893, 'r': 32, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 5 with value: 0.7260868549346924.
Trial 6 finished with value: 0.7556001543998718 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 10, 'learning_rate': 5.3493541463695194e-05, 'r': 16, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}. Best is trial 5 with value: 0.7260868549346924.
Trial 7 finished with value: 0.8446041345596313 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 7, 'learning_rate': 5.326908191253073e-05, 'r': 64, 'lora_alpha': 16, 'target_modules': ['q_proj', 'k_proj']}. Best is trial 5 with value: 0.7260868549346924.
Trial 8 finished with value: 0.7253598570823669 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 0, 'learning_rate': 0.00035661001923737655, 'r': 8, 'lora_alpha': 64, 'target_modules': ['v_proj', 'k_proj']}. Best is trial 8 with value: 0.7253598570823669.
Trial 9 finished with value: 0.7464359402656555 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 7, 'learning_rate': 0.00012888184615615113, 'r': 16, 'lora_alpha': 16, 'target_modules': ['v_proj', 'k_proj']}. Best is trial 8 with value: 0.7253598570823669.
 Trial 10 finished with value: 0.7841811180114746 and parameters: {'per_device_train_batch_size': 9, 'warmup_steps': 0, 'learning_rate': 0.0009095231413068411, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'k_proj']}. Best is trial 8 with value: 0.7253598570823669.
Trial 11 finished with value: 0.7339389324188232 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 5, 'learning_rate': 0.00018764919122003519, 'r': 32, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}. Best is trial 8 with value: 0.7253598570823669.
Trial 12 finished with value: 0.7283551096916199 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 6, 'learning_rate': 0.0009850723354302667, 'r': 8, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 8 with value: 0.7253598570823669.
Trial 13 finished with value: 0.7337584495544434 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 5, 'learning_rate': 0.00021916681928189837, 'r': 8, 'lora_alpha': 8, 'target_modules': ['v_proj', 'k_proj']}. Best is trial 8 with value: 0.7253598570823669.
Trial 14 finished with value: 0.7234758138656616 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 8, 'learning_rate': 0.0004788578660188426, 'r': 32, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 14 with value: 0.7234758138656616.
Trial 15 finished with value: 1.1216356754302979 and parameters: {'per_device_train_batch_size': 10, 'warmup_steps': 9, 'learning_rate': 1.415716857725099e-05, 'r': 32, 'lora_alpha': 32, 'target_modules': ['v_proj', 'k_proj']}. Best is trial 14 with value: 0.7234758138656616.
Trial 16 finished with value: 0.7237014770507812 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 4, 'learning_rate': 0.0007233760469270178, 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}. Best is trial 14 with value: 0.7234758138656616.
Trial 17 finished with value: 0.7210278511047363 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 4, 'learning_rate': 0.000649330450809472, 'r': 64, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}. Best is trial 17 with value: 0.7210278511047363.
Trial 18 finished with value: 0.7210537791252136 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 8, 'learning_rate': 0.0006062460984301994, 'r': 64, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}. Best is trial 17 with value: 0.7210278511047363.
Trial 19 finished with value: 0.729016125202179 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 5, 'learning_rate': 0.00020522232426404557, 'r': 64, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}. Best is trial 17 with value: 0.7210278511047363.

Best hyperparameters: {'per_device_train_batch_size': 5, 'warmup_steps': 4, 'learning_rate': 0.000649330450809472, 'r': 64, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}