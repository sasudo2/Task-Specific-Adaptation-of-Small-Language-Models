 Trial 0 finished with value: 1.4871132373809814 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 3, 'learning_rate': 3.986811184660196e-05, 'r': 64, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 1 finished with value: 0.7335254549980164 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 0, 'learning_rate': 0.00037003809984736415, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
 Trial 2 finished with value: 0.7669356465339661 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 3, 'learning_rate': 9.938397041096432e-05, 'r': 64, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 3 finished with value: 1.8330204486846924 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 4, 'learning_rate': 1.141819103975943e-05, 'r': 64, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 4 finished with value: 1.5336822271347046 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 2, 'learning_rate': 6.306286946788976e-05, 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 5 finished with value: 0.7575558423995972 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 2, 'learning_rate': 0.0001729630881709539, 'r': 64, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
 Trial 6 finished with value: 1.4349465370178223 and parameters: {'per_device_train_batch_size': 5, 'warmup_steps': 3, 'learning_rate': 5.589217379428488e-05, 'r': 32, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 7 finished with value: 1.5084116458892822 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 4, 'learning_rate': 0.00014796060354946012, 'r': 8, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 8 finished with value: 1.761817216873169 and parameters: {'per_device_train_batch_size': 7, 'warmup_steps': 0, 'learning_rate': 1.4948347198607013e-05, 'r': 32, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 9 finished with value: 1.321707010269165 and parameters: {'per_device_train_batch_size': 2, 'warmup_steps': 1, 'learning_rate': 5.047576253783951e-05, 'r': 64, 'lora_alpha': 8, 'target_modules': ['q_proj', 'v_proj', 'k_proj']}.
 Trial 10 finished with value: 0.7399393916130066 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 0, 'learning_rate': 0.0007841189036350719, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 11 finished with value: 0.7607496380805969 and parameters: {'per_device_train_batch_size': 8, 'warmup_steps': 0, 'learning_rate': 0.0008366260949083839, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 12 finished with value: 0.7396601438522339 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 1, 'learning_rate': 0.0007890996448116656, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 13 finished with value: 0.8433547019958496 and parameters: {'per_device_train_batch_size': 8, 'warmup_steps': 1, 'learning_rate': 0.0003377745931750341, 'r': 16, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 14 finished with value: 0.7628464698791504 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 1, 'learning_rate': 0.0004321841805049895, 'r': 8, 'lora_alpha': 16, 'target_modules': ['q_proj', 'v_proj']}
  Trial 15 finished with value: 0.7330776453018188 and parameters: {'per_device_train_batch_size': 6, 'warmup_steps': 5, 'learning_rate': 0.0003443636570165762, 'r': 16, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
  Trial 16 finished with value: 0.7166050672531128 and parameters: {'per_device_train_batch_size': 4, 'warmup_steps': 5, 'learning_rate': 0.00031287503882560544, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}
   Trial 17 finished with value: 0.7194110751152039 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 5, 'learning_rate': 0.00021151571890294502, 'r': 16, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
   Trial 18 finished with value: 0.7313404083251953 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 5, 'learning_rate': 0.00014198820139959268, 'r': 8, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}.
    Trial 19 finished with value: 0.7131057977676392 and parameters: {'per_device_train_batch_size': 3, 'warmup_steps': 4, 'learning_rate': 0.00026491674059880415, 'r': 32, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}. Best is trial 19 with value: 0.7131057977676392.


    Best hyperparameters: {'per_device_train_batch_size': 3, 'warmup_steps': 4, 'learning_rate': 0.00026491674059880415, 'r': 32, 'lora_alpha': 64, 'target_modules': ['q_proj', 'v_proj']}