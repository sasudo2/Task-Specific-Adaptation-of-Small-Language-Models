\chapter{Methodology}
\begin{normaltext}
This project aims to fine-tune Llama 2 7B large language model for Python code generation. 
The methodology is structured in three phases: dataset preparation, fine-tuning, and evaluation. 
Figure~\ref{fig:flowchart} shows the flowchart of the fine-tuning procedure.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{flowchart.png} % Uncomment when you add the figure
    \caption{Flowchart of the fine-tuning process}
    \label{fig:flowchart}
\end{figure}

% ---------------------------------------------------------
\section{Dataset Description and Preparation}

Fine-tuning Llama 2 7B for Python code generation requires a high-quality, domain-specific dataset 
containing diverse programming tasks and their corresponding solutions. 
We will construct a composite dataset from multiple reputable open-source sources.
\end{normaltext}
\subsection{Dataset Sources and Composition:}
\begin{normaltext}
\begin{enumerate}
\item{ FlyTech Python Code Dataset (2025)}~
\begin{itemize}
    \item {Size:} $\sim$42,000 instruction--code pairs
    \item {Content:} Python scripts with comments, docstrings, and structured tasks
    \item {Reason for Selection:}
    \begin{itemize}
        \item Curated specifically for code generation
        \item High-quality, well-commented Python code
        \item Diverse mix of algorithmic and real-world examples
    \end{itemize}
    \item {License:} MIT License (per Hugging Face repository)
\end{itemize}

    \item Python Code Instructions 18k (Alpaca-style)
        \begin{itemize}
            \item {Size:} $\sim$18,000 instruction--response pairs
            \item {Content:} Instruction prompts and Python code responses
            \item {Reason for Selection:}
            \begin{itemize}
                \item Matches the instruction-tuning structure required by OPT
                \item Provides human-readable prompts with explanations
                \item Complements FlyTech by increasing prompt diversity
            \end{itemize}
            \item {License:} CC BY-NC 4.0 (non-commercial research use)
        \end{itemize}
    
    \item LeetCode/HackerRank Programming Problems (Evaluation Only)
        \begin{itemize}
            \item {Size:} $\sim$200--300 problems (not used for training)
            \item {Content:} Problem descriptions + reference solutions written by us
            \item {Reason for Selection:}
                \begin{itemize}
                    \item Realistic competitive programming tasks
                    \item Enables functional correctness testing
                \end{itemize}
            \item {License:}
                \begin{itemize}
                    \item Direct redistribution of LeetCode content is not allowed
                    \item Only prompts used for evaluation (fair use), with our own solutions
                \end{itemize}
        \end{itemize}

    
    
\end{enumerate}
  We will standardize the problem set used from LeetCode/HackerRank and not directly extract their internal datasets.  
\end{normaltext}


\subsection{Dataset Processing Pipeline:}
\begin{normaltext}
    The collected dataset will undergo several preprocessing steps to ensure quality and consistency before training:
    \begin{itemize}
        \item Duplicate removal
        All repeated or identical instruction–code pairs will be removed to eliminate redundancy and reduce overfitting.

        \item Filtering Incomplete Python Code:
        Examples containing syntactically incorrect or incomplete Python code will be discarded using rule-based checks and code-parsing techniques.
        \item Adding Chain-of-Thought (CoT) Explanations:
        Each example will be enriched with a brief chain-of-thought explanation describing the reasoning steps used to solve the instruction.This helps the model learn structured reasoning patterns.
        \item Normalization and Formatting:
        All samples will be standardized with consistent indentation, spacing, variable naming, and overall formatting.
    \end{itemize}
    {Final Dataset Size (Expected)}
    \begin{itemize}
        \item FlyTech: $\sim$40,000
        \item Alpaca: $\sim$17,000
        \item {Total usable examples:} $\sim$57,000
    \end{itemize}
\end{normaltext}


\subsection{Train/Validation Split:}
\begin{normaltext}
We will divide the cleaned dataset into:
\begin{itemize}
    \item 90\% training set
    \item 5\% validation set
    \item 5\% evaluation set
\end{itemize}

The evaluation set (LeetCode/HackerRank) remains entirely separate.
\end{normaltext}


\subsection{Instruction Formatting and Tokenization}
\begin{normaltext}
    Each training example will be converted into an instruction-style structure:

    \begin{quote}
    \begin{ttfamily}
    Instruction: Write a Python code for printing whether a given number is Armstrong.

    Chain-of-Thought: Check if a number equals the sum of its digits each raised to the power of the number of digits.

    Response:
    \begin{verbatim}
    num = int(input("Enter a number: "))
    digits = len(str(num))
    armstrong_sum = sum(int(d)**digits for d in str(num))
    if num == armstrong_sum:
        print(f"{num} is an Armstrong number")
    else:
        print(f"{num} is not an Armstrong number")
    \end{verbatim}
    \end{ttfamily}
    \end{quote}

    Finally, all training examples will be tokenized using the Llama tokenizer to ensure full compatibility with the pretrained Llama 2 model.
     This guarantees that special tokens, vocabulary indices, and formatting structures align correctly during QLoRA fine-tuning, preventing token-mismatch issues and maintaining optimal model performance.

\end{normaltext}

% ---------------------------------------------------------
\section{Model Fine-Tuning}
\begin{normaltext}
We will fine-tune the Llama 2 model using QLoRA to enable efficient Python code generation from textual prompts. 
The base model will be loaded with pretrained and 4-bit quantized weights to reduce memory usage.
\end{normaltext}

\subsection{Fine-Tuning Approach}
\begin{normaltext}
Fine-tuning follows the instruction-tuning paradigm: given a prompt x, the model predicts the next token in the target Python code sequence y.
To improve reasoning quality and reduce hallucination, we additionally include Chain-of-Thought (CoT) annotations inside the training samples. Each training example consists of:
Instruction
Human-written reasoning (CoT)
Final code output
This encourages the model to learn step-by-step reasoning patterns before producing the final answer.
Mathematically, the model maximizes the likelihood of the entire sequence consisting of instruction, reasoning, and output:


\begin{equation}
\text{loss} = -\sum_{t=1}^{T} \log P_{\theta} (y_{t}^{\text{true}} \mid y_{<t}, x)
\end{equation}

Model parameters are updated using gradient descent:

\begin{equation}
\theta_{i+1} \leftarrow \theta_i - \alpha \frac{\partial (\text{loss})}{\partial \theta}
\end{equation}

CoT Integration
The CoT component explicitly teaches the model:
how to plan the solution
how to reason before generating code
This approach is known to significantly reduce hallucinations and improve generalization in small models.

% ---------------------------------------------------------
\subsection{QLoRA for Efficient Training}

QLoRA enables parameter-efficient fine-tuning by:
\begin{itemize}
    \item Keeping the pretrained model weights frozen
    \item Loading weights in 4-bit quantization
    \item Adding trainable low-rank adapter matrices inside transformer blocks
\end{itemize}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\linewidth]{without_qlora.png} % Uncomment when you add the figure
%     \caption{Base Model Finetuning with out QLoRA Adapter Layers}
%     \label{fig:Base Model Finetuning with out QLoRA Adapter Layer}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{with_qlora.png} 
    % Uncomment when you add the figure
    \caption{Base Model Finetuning with QLoRA Adapter Layers}
    \label{fig:Base Model Finetuning with QLoRA Adapter Layersg}
\end{figure}

Only the adapter matrices are optimized.This allows us to incorporate CoT-augmented samples without increasing memory usage, making training feasible in Google Colab.
CoT pairs tend to be longer, but QLoRA’s memory-efficient structure allows handling large token sequences even on limited hardware.

\end{normaltext}
% ---------------------------------------------------------
\section{Training Setup}
\begin{normaltext}
    \begin{itemize}
        \item \textbf{Platform:} Google Colab
        \item \textbf{Optimizer:} AdamW
        \item \textbf{Hyperparameters:} learning rate, batch size, and epochs tuned using validation loss
        \item \textbf{Regularization:} Dropout applied to adapter layers
        \item \textbf{Early stopping:} Enabled based on validation loss
        \item \textbf{Batching:} Token-based batching, using the maximum number of tokens that fit in GPU memory (typically 8k–32k tokens per batch on Colab)
        \item \textbf{Multiple Session:} If needed, training will be split across multiple Colab sessions with checkpointing
    \end{itemize}


\vspace{18 pt}
\section{Monitoring and Checkpointing} %########write mor here########

\begin{itemize}
    \item Training monitored with Weights \& Biases (WandB)
    \item Periodic checkpoints stored locally and on Google Drive
    \item Final model uploaded to Hugging Face Hub
\end{itemize}
\end{normaltext}


% ---------------------------------------------------------
\section{Evaluation}
\begin{normaltext}
The fine-tuned model will be compared against the base Llama 2 7B model using two evaluation strategies.
\begin{enumerate}
    \item{Functional Correctness (Primary Evaluation)}

        Generated solutions will be executed in a secure Docker-based sandbox. Each result is classified as:

        \begin{itemize}
            \item \textbf{Right:} Correct output for all test cases
            \item \textbf{Partial:} Runs but fails some test cases
            \item \textbf{Wrong:} Crashes or produces incorrect output
        \end{itemize}

        This directly measures true problem-solving ability.

    \item{Automatic Similarity Metrics (BLEU, CodeBLEU)}

        \begin{itemize}
            \item \textbf{BLEU:} Evaluates token-level similarity to reference solutions
            \item \textbf{CodeBLEU:} Incorporates syntax, data-flow, and semantic structure; more suitable for code generation
        \end{itemize}

        These are used as supporting quantitative metrics alongside functional evaluation.

    \item{Combined Evaluation Insight}

        By combining functional execution with similarity-based metrics, we ensure that the generated Python code is not only syntactically valid but also meaningful and semantically aligned with correct solutions.
\end{enumerate}

\end{normaltext}