\chapter{Methodology}
\begin{normaltext}
This project aims to fine-tune the OPT-350M large language model for Python code generation. The methodology is structured in three phases: dataset preparation, fine-tuning, and evaluation. 
Figure~\ref{fig:methodology_flowchart} shows the flowchart of the fine-tuning procedure.
\end{normaltext}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{finetuning_flowchart.png} %Uncomment when you add the figure
    \caption{Flowchart of the fine-tuning process}
    \label{fig:methodology_flowchart}
\end{figure}

% ---------------------------------------------------------
\section{Dataset Description and Preparation}

\begin{normaltext}
Fine-tuning OPT-350M for Python code generation requires a high-quality, domain-specific dataset containing diverse programming tasks and their corresponding solutions. We will construct a composite dataset from multiple reputable open-source sources.
\end{normaltext}

\subsection{Dataset Sources and Composition:}
\begin{enumerate}
    \begin{normaltext}
    \item FlyTech Python Code Dataset (2025)
        \begin{itemize}
            \item {Size:} $\sim$42,000 instruction--code pairs
            \item {Content:} Python scripts with comments, docstrings, and structured tasks
            \item {Reason for Selection:}
            \begin{itemize}
                \item Curated specifically for code generation
                \item High-quality, well-commented Python code
                \item Diverse mix of algorithmic and real-world examples
            \end{itemize}
            \item {License:} MIT License (per Hugging Facere pository)
        \end{itemize}

    \item Python Code Instructions 18k (Alpaca-style)
        \begin{itemize}
            \item {Size:} $\sim$18,000 instruction--response pairs
            \item {Content:} Instruction prompts and Python code responses
            \item {Reason for Selection:}
            \begin{itemize}
                \item Matches the instruction-tuning structure required by OPT
                \item Provides human-readable prompts with explanations
                \item Complements FlyTech by increasing prompt diversity
            \end{itemize}
            \item {License:} CC BY-NC 4.0 (non-commercial research use)
        \end{itemize}
    
    \item LeetCode/HackerRank Programming Problems (Evaluation Only)
        \begin{itemize}
            \item {Size:} $\sim$200--300 problems (not used for training)
            \item {Content:} Problem descriptions + reference solutions written by us
            \item {Reason for Selection:}
                \begin{itemize}
                    \item Realistic competitive programming tasks
                    \item Enables functional correctness testing
                \end{itemize}
            \item {License:}
                \begin{itemize}
                    \item Direct redistribution of LeetCode content is not allowed
                    \item Only prompts used for evaluation (fair use), with our own solutions
                \end{itemize}
        \end{itemize}
     \end{normaltext}
\end{enumerate}
\begin{normaltext}
  We will standardize the problem set used from LeetCode/HackerRank and not directly extract their internal datasets.  
\end{normaltext}


\subsection{Dataset Cleaning:}
\begin{normaltext}
    Collected datasets will go through the following processing steps:
    \begin{itemize}
        \item Removal of duplicate examples
        \item Filtering of incomplete or non-functional Python code
        \item Normalization of formatting and consistent indentation
    \end{itemize}
    {Final Dataset Size (Expected)}
    \begin{itemize}
        \item FlyTech: $\sim$40,000
        \item Alpaca: $\sim$17,000
        \item {Total usable examples:} $\sim$57,000
    \end{itemize}
\end{normaltext}


\subsection{Train/Validation Split:}
\begin{normaltext}
We will divide the cleaned dataset into:
\begin{itemize}
    \item 90\% training set
    \item 5\% validation set
    \item 5\% evaluation set
\end{itemize}

The evaluation set (LeetCode/HackerRank) remains entirely separate.
\end{normaltext}


\subsection{Instruction Formatting and Tokenization}
\begin{normaltext}
    Each training example will be converted into an instruction-style structure:

    \begin{quote}
    \begin{ttfamily}
    Instruction: Write a Python code for printing Hello World.\\
    Response: <Python code with docstrings or comments>
    \end{ttfamily}
    \end{quote}

    Tokenization will be performed using the official OPT tokenizer to ensure compatibility with the pretrained model.
\end{normaltext}

% ---------------------------------------------------------
\section{Model Fine-Tuning}
\begin{normaltext}
We will fine-tune the OPT-350M model \cite{Zhang2022} using QLoRA to enable efficient code generation while minimizing computational cost. 
The base model will be loaded in quantized form and kept frozen.
\end{normaltext}

\subsection{Fine-Tuning Approach}
\begin{normaltext}
Fine-tuning follows an instruction-tuning paradigm: given a text prompt $x$, the model predicts the next token in the Python code sequence $y$.

\begin{equation}
\text{loss} = -\sum_{t=1}^{T} \log P_{\theta} (y_{t}^{\text{true}} \mid y_{<t}, x)
\end{equation}

Model parameters are updated using gradient descent:

\begin{equation}
\theta_{i+1} \leftarrow \theta_i - \alpha \frac{\partial (\text{loss})}{\partial \theta}
\end{equation}

% ---------------------------------------------------------
\subsection{QLoRA for Efficient Training}

QLoRA enables parameter-efficient fine-tuning by:
\begin{itemize}
    \item Keeping the pretrained model weights frozen
    \item Loading weights in 4-bit quantization
    \item Adding trainable low-rank adapter matrices inside transformer blocks
\end{itemize}

Only the adapter parameters are optimized, reducing VRAM usage and making training feasible on Google Colab.
\end{normaltext}
% ---------------------------------------------------------
\section{Training Setup}
\begin{normaltext}
    \begin{itemize}
        \item \textbf{Platform:} Google Colab
        \item \textbf{Optimizer:} AdamW
        \item \textbf{Hyperparameters:} learning rate, batch size, and epochs tuned using validation loss
        \item \textbf{Regularization:} Dropout applied to adapter layers
        \item \textbf{Early stopping:} Enabled based on validation loss
        \item \textbf{Batching:} Token-based batching targeting $\sim$0.5M tokens per batch
    \end{itemize}

If a single Colab session is insufficient for training the full dataset, the dataset will be divided into multiple segments and trained incrementally, with checkpoints saved between sessions.

\vspace{18 pt}
\subsubsection{Monitoring and Checkpointing}

\begin{itemize}
    \item Training monitored with Weights \& Biases (WandB)
    \item Periodic checkpoints stored locally and on Google Drive
    \item Final model uploaded to Hugging Face Hub
\end{itemize}
\end{normaltext}


% ---------------------------------------------------------
\section{Evaluation}
\begin{normaltext}
The fine-tuned model will be compared against the base OPT-350M model using three evaluation strategies.
\begin{enumerate}
    \item{Functional Correctness (Primary Evaluation)}

        Generated solutions will be executed in a secure Docker-based sandbox. Each result is classified as:

        \begin{itemize}
            \item \textbf{Right:} Correct output for all test cases
            \item \textbf{Partial:} Runs but fails some test cases
            \item \textbf{Wrong:} Crashes or produces incorrect output
        \end{itemize}

        This directly measures true problem-solving ability.

    \item{Automatic Similarity Metrics (BLEU, CodeBLEU)}

        \begin{itemize}
            \item \textbf{BLEU:} Evaluates token-level similarity to reference solutions
            \item \textbf{CodeBLEU:} Incorporates syntax, data-flow, and semantic structure; more suitable for code generation
        \end{itemize}

        These are used as supporting quantitative metrics alongside functional evaluation.

    \item{Combined Evaluation Insight}

        By combining functional execution with similarity-based metrics, we ensure that the generated Python code is not only syntactically valid but also meaningful and semantically aligned with correct solutions.
\end{enumerate}

\end{normaltext}