\chapter{Litrature Review}
\begin{normaltext}
Deploying large language models on resource-constrained devices requires techniques that dramatically reduce computational and memory requirements while maintaining performance. This review examines recent advances in parameter-efficient fine-tuning (PEFT) that enable model adaptation for low-specification environments.

\vspace{18pt}
\section{Low Rank Adaptation (LoRA)}
Hu et al.\cite{Hu2021} introduced LoRA, which freezes pre-trained model weights and injects trainable rank decomposition matrices into each Transformer layer. This approach reduces trainable parameters by 10,000 times and GPU memory requirements by three-fold compared to full fine-tuning of GPT-3 175B, enabling efficient adaptation on modest hardware without significant performance degradation.

\vspace{18pt}
\section{Quantized Low Rank Adaptation (QLoRA)}
Dettmers et al.\cite{Dettmers2023} extended LoRA by combining quantization with low-rank adaptation. QLoRA introduces 4-bit NormalFloat (NF4), Double Quantization, and Paged Optimizers to achieve further memory reduction. The approach achieved 99.3\% of ChatGPT's performance with only 24 hours of single-GPU training, demonstrating that small, high-quality datasets are sufficient for state-of-the-art results when paired with efficient fine-tuning techniques.

\vspace{18pt}
\section{Open Pre-trained Transformer (OPT)}
Zhang et al. \cite{Zhang2022} released Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained models ranging from 125M to 175B parameters with full model weights made publicly available. OPT-175B achieves comparable performance to GPT-3 while requiring only one-seventh of the carbon footprint to develop. The availability of these open models with complete access to weights addresses the reproducibility challenges in large language model research and provides accessible base models for adaptation techniques.

\vspace{18pt}
\section{Domain-Specific Application}
The practical effectiveness of PEFT techniques is demonstrated in fine-tuning CodeLlama-7B for Fortran code generation \cite{Govande2024}. Using LoRA to adapt only attention weights while freezing MLP modules, the researchers fine-tuned the model on high-quality Fortran code from public repositories and NASA codebases, with GPT-3.5-generated descriptions. The fine-tuned model significantly outperformed vanilla CodeLlama-7B-Instruct, achieving 41\% improvement in compilation rate, 150\% improvement in execution rate, and 75\% improvement in correct output rate on 540 LeetCode problems.



\end{normaltext}
