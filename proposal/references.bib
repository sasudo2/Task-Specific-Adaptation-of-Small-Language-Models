@article{Hu2021,
  author = {Hu, Edward and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal = {arXiv preprint arXiv:2106.09685},
  year = {2021},
  url = {https://arxiv.org/abs/2106.09685}
}

@article{Dettmers2023,
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title = {QLoRA: Efficient Finetuning of Quantized LLMs},
  journal = {arXiv preprint arXiv:2305.14314},
  year = {2023},
  url = {https://arxiv.org/abs/2305.14314}
}

@article{Govande2024,
  author = {Govande, Soham and Kang, Taeuk and Shi, Andrew},
  title = {Fine-tuning CodeLlama-7B on Synthetic Training Data for Fortran Code Generation using PEFT},
  journal = {Stanford CS224N Custom Project},
  year = {2024},
  school = {Stanford University}
}

@article{Zhang2022,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}