@article{Hu2021,
  author = {Hu, Edward and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal = {arXiv preprint arXiv:2106.09685},
  year = {2021},
  url = {https://arxiv.org/abs/2106.09685}
}

@article{Dettmers2023,
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title = {QLoRA: Efficient Finetuning of Quantized LLMs},
  journal = {arXiv preprint arXiv:2305.14314},
  year = {2023},
  url = {https://arxiv.org/abs/2305.14314}
}

@article{Govande2024,
  author = {Govande, Soham and Kang, Taeuk and Shi, Andrew},
  title = {Fine-tuning CodeLlama-7B on Synthetic Training Data for Fortran Code Generation using PEFT},
  journal = {Stanford CS224N Custom Project},
  year = {2024},
  school = {Stanford University}
}

@article{Zhang2022,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{Touvron2023Llama2,
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year = {2023},
  url = {https://arxiv.org/abs/2307.09288},
  note = {Meta AI}
}