{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd39a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dipst/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available files:\n",
      "  ALL/test-00000-of-00001.parquet\n",
      "  ALL/train-00000-of-00009.parquet\n",
      "  ALL/train-00001-of-00009.parquet\n",
      "  ALL/train-00002-of-00009.parquet\n",
      "  ALL/train-00003-of-00009.parquet\n",
      "  ALL/train-00004-of-00009.parquet\n",
      "  ALL/train-00005-of-00009.parquet\n",
      "  ALL/train-00006-of-00009.parquet\n",
      "  ALL/train-00007-of-00009.parquet\n",
      "  ALL/train-00008-of-00009.parquet\n",
      "  test/data-00000-of-00001.arrow\n",
      "  train/data-00000-of-00009.arrow\n",
      "  train/data-00001-of-00009.arrow\n",
      "  train/data-00002-of-00009.arrow\n",
      "  train/data-00003-of-00009.arrow\n",
      "  train/data-00004-of-00009.arrow\n",
      "  train/data-00005-of-00009.arrow\n",
      "  train/data-00006-of-00009.arrow\n",
      "  train/data-00007-of-00009.arrow\n",
      "  train/data-00008-of-00009.arrow\n",
      "\n",
      "Downloading 20 files...\n",
      "✓ Downloaded: ALL/test-00000-of-00001.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/test-00000-of-00001.parquet\n",
      "✓ Downloaded: ALL/train-00000-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00000-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00001-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00001-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00002-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00002-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00003-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00003-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00004-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00004-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00005-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00005-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00006-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00006-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00007-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00007-of-00009.parquet\n",
      "✓ Downloaded: ALL/train-00008-of-00009.parquet\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL/train-00008-of-00009.parquet\n",
      "✓ Downloaded: test/data-00000-of-00001.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/test/data-00000-of-00001.arrow\n",
      "✓ Downloaded: train/data-00000-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00000-of-00009.arrow\n",
      "✓ Downloaded: train/data-00001-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00001-of-00009.arrow\n",
      "✓ Downloaded: train/data-00002-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00002-of-00009.arrow\n",
      "✓ Downloaded: train/data-00003-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00003-of-00009.arrow\n",
      "✓ Downloaded: train/data-00004-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00004-of-00009.arrow\n",
      "✓ Downloaded: train/data-00005-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00005-of-00009.arrow\n",
      "✓ Downloaded: train/data-00006-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00006-of-00009.arrow\n",
      "✓ Downloaded: train/data-00007-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00007-of-00009.arrow\n",
      "✓ Downloaded: train/data-00008-of-00009.arrow\n",
      "  Location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/train/data-00008-of-00009.arrow\n",
      "\n",
      "================================================================================\n",
      "DOWNLOAD SUMMARY:\n",
      "================================================================================\n",
      "Total files downloaded: 20\n",
      "Base location: /home/dipst/.cache/huggingface/hub/datasets--BAAI--TACO/snapshots/d593ed0a2becbbc952230bb89be09189bf1056dc/ALL\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from huggingface_hub import list_repo_tree\n",
    "try:\n",
    "    files = list_repo_tree(\"BAAI/TACO\", repo_type=\"dataset\", recursive=True)\n",
    "    print(f\"\\nAvailable files:\")\n",
    "    file_list = [file.path for file in files if file.path.endswith(('.arrow', '.parquet'))]\n",
    "    for file_path in file_list[:20]:  # Show first 20\n",
    "        print(f\"  {file_path}\")\n",
    "    \n",
    "    # Download all arrow files\n",
    "    print(f\"\\nDownloading {len(file_list)} files...\")\n",
    "    downloaded_paths = []\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            local_path = hf_hub_download(\n",
    "                repo_id=\"BAAI/TACO\",\n",
    "                filename=file_path,\n",
    "                repo_type=\"dataset\"\n",
    "            )\n",
    "            downloaded_paths.append(local_path)\n",
    "            print(f\"✓ Downloaded: {file_path}\")\n",
    "            print(f\"  Location: {local_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to download {file_path}: {e}\")\n",
    "    \n",
    "    # Show summary of download locations\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DOWNLOAD SUMMARY:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total files downloaded: {len(downloaded_paths)}\")\n",
    "    if downloaded_paths:\n",
    "        print(f\"Base location: {os.path.dirname(downloaded_paths[0])}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in alternative approach: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a3777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8e2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV written to: /mnt/c/users/dipst/OneDrive/Desktop/task_specific_adaptation/Notebooks/taco_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "output_path = \"taco_cleaned.csv\"\n",
    "\n",
    "# If CSV already exists, do not overwrite; skip writing\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"CSV already exists, skipping write: {os.path.abspath(output_path)}\")\n",
    "else:\n",
    "    parquet_downloaded_paths = downloaded_paths[:10]\n",
    "    header_written = False\n",
    "\n",
    "    def has_solution(v) -> bool:\n",
    "        if isinstance(v, list):\n",
    "            return len(v) > 0\n",
    "        try:\n",
    "            parsed = ast.literal_eval(v)\n",
    "            if isinstance(parsed, list):\n",
    "                return len(parsed) > 0\n",
    "        except Exception:\n",
    "            pass\n",
    "        return v not in (\"[]\", \"\", None)\n",
    "\n",
    "    def extract_first_solution(val):\n",
    "        try:\n",
    "            if isinstance(val, list):\n",
    "                return val[0] if val else None\n",
    "            parsed = ast.literal_eval(val)\n",
    "            return parsed[0] if parsed else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    for path in parquet_downloaded_paths:\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df.drop([\n",
    "            \"starter_code\", \"name\", \"source\", \"skill_types\", \"url\",\n",
    "            \"Expected Auxiliary Space\", \"input_output\", \"time_limit\", \"raw_tags\", \"date\",\n",
    "            \"picture_num\", \"memory_limit\", \"Expected Time Complexity\"\n",
    "        ], axis=1, errors=\"ignore\")\n",
    "\n",
    "        # Filter rows with non-empty solutions and normalize to first entry\n",
    "        df = df[df[\"solutions\"].apply(has_solution)].reset_index(drop=True)\n",
    "        df[\"solutions\"] = df[\"solutions\"].apply(extract_first_solution)\n",
    "\n",
    "        # Write directly to CSV, appending after the first chunk\n",
    "        df.to_csv(output_path, mode=\"a\", header=not header_written, index=False)\n",
    "        header_written = True\n",
    "\n",
    "    print(f\"CSV written to: {os.path.abspath(output_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ea6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/mnt/c/users/dipst/OneDrive/Desktop/task_specific_adaptation/Notebooks/taco_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6c3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['question','solutions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6231b362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20493, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
