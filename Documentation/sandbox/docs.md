# ðŸ— Task-Specific Evaluation Sandbox for Code Generation

  **sandbox environment to evaluate code generated by language models** on LeetCode-style problems, with Docker-based isolation and automatic logging of correctness and performance.

---

## ðŸ”¹ Overview

The pipeline:

1. **Problem set** â†’ JSON file containing problem statements, function signatures, and test cases.
2. **Code generation** â†’ Model generates candidate solutions.
3. **Sandbox execution** â†’ Docker runs each solution safely with time/memory limits.
4. **Evaluation** â†’ Checks correctness, runtime, memory usage.
5. **Logging** â†’ Results saved in JSON for analysis.

---

## ðŸ”¹ Architecture Diagram

```mermaid
flowchart TD
    A[leetcode_eval_set.json] --> B[generate_code(prompt)]
    B --> C[code_extractor.py]
    C --> D[Docker Sandbox]
    D --> E[run test cases]
    E --> F[Evaluate correctness & runtime]
    F --> G[results.json]
